{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140759fc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4db0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import heapq\n",
    "from typing import List, Optional\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c8599",
   "metadata": {},
   "source": [
    "### Load data from the merged TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f640755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\AppData\\Local\\Temp\\ipykernel_5928\\3055693431.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merging_tsv_files_data = pd.read_csv('..\\\\data\\\\merging_tsv_files_data.csv', sep=';')\n"
     ]
    }
   ],
   "source": [
    "merging_tsv_files_data = pd.read_csv('..\\\\data\\\\merging_tsv_files_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e1173",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee695ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_top3_prior(group: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each row in an actor group, emit the top-3 *prior* ratings (and titles) before this row.\n",
    "    Uses a size-3 min-heap for O(n) per group.\n",
    "    \"\"\"\n",
    "    top_heap = []  # min-heap of (rating, title) so smallest on top; keep size<=3\n",
    "    prior1_rating, prior2_rating, prior3_rating = [], [], []\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        # snapshot current top prior works (sorted desc)\n",
    "        snapshot = sorted(top_heap, key=lambda x: x[0], reverse=True)\n",
    "        ratings = [r for r, _ in snapshot]\n",
    "\n",
    "        # pad to length 3\n",
    "        while len(ratings) < 3:\n",
    "            ratings.append(np.nan)\n",
    "\n",
    "        prior1_rating.append(ratings[0])\n",
    "        prior2_rating.append(ratings[1])\n",
    "        prior3_rating.append(ratings[2])\n",
    "        # now add current row into the actor's history\n",
    "        heapq.heappush(top_heap, (row['movie_score'], row.get('tconst', None)))\n",
    "        if len(top_heap) > 3:\n",
    "            heapq.heappop(top_heap)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'prior1_rating': prior1_rating,\n",
    "        'prior2_rating': prior2_rating,\n",
    "        'prior3_rating': prior3_rating,\n",
    "    }, index=group.index)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f48dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_movie_actor_wide(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    movie_id_col: str = \"tconst\",\n",
    "    actor_order_col: str = \"ordering\",\n",
    "    max_actors: int = 15,\n",
    "    actor_categories: tuple = (\"actor\", \"actress\", 'self'),\n",
    "    include_actor_text_cols: bool = True,\n",
    "    primary_name_col: str = \"primaryName\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a long movie-actor dataset to a wide one: 1 row per movie, N actors as columns.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # best-effort auto-detect for primary name column if exact match isn't present\n",
    "    if primary_name_col not in df.columns:\n",
    "        candidates = [c for c in df.columns if c.lower().replace(\"_\",\"\") == \"primaryname\"]\n",
    "        if len(candidates) == 1:\n",
    "            primary_name_col = candidates[0]\n",
    "\n",
    "    # Ensure ordering exists\n",
    "    if actor_order_col not in df.columns:\n",
    "        df[actor_order_col] = df.groupby(movie_id_col).cumcount() + 1\n",
    "\n",
    "    # Filter to actors only (if category exists)\n",
    "    if \"category\" in df.columns and actor_categories:\n",
    "        df = df[df[\"category\"].isin(actor_categories)].copy()\n",
    "\n",
    "    # Sort so 'first()' is first-billed for movie-level fields\n",
    "    df = df.sort_values([movie_id_col, actor_order_col], kind=\"mergesort\")\n",
    "\n",
    "    # Define movie-level columns (keep once per movie)\n",
    "    actor_level_cols = {\n",
    "        \"nconst\", primary_name_col,\n",
    "        \"actor_sentiment\", \"prior_movie_actor_sentiment\",\n",
    "        \"prior1_rating\", \"prior2_rating\", \"prior3_rating\",\n",
    "        actor_order_col\n",
    "    }\n",
    "    movie_level_cols = [c for c in df.columns if c not in actor_level_cols]\n",
    "\n",
    "    # Base movie table\n",
    "    movie_base = df.groupby(movie_id_col, as_index=False)[movie_level_cols].first()\n",
    "\n",
    "    # Assign actor index and cap at N\n",
    "    df[\"actor_idx\"] = df.groupby(movie_id_col).cumcount() + 1\n",
    "    df = df[df[\"actor_idx\"] <= max_actors].copy()\n",
    "\n",
    "    # Select actor features\n",
    "    actor_feature_cols = [\n",
    "        \"actor_sentiment\",\n",
    "        \"prior_movie_actor_sentiment\",\n",
    "        \"prior1_rating\", \"prior2_rating\", \"prior3_rating\",\n",
    "    ]\n",
    "    if include_actor_text_cols:\n",
    "        actor_feature_cols = [\"nconst\", primary_name_col] + actor_feature_cols\n",
    "\n",
    "    keep_cols = [movie_id_col, \"actor_idx\"] + [c for c in actor_feature_cols if c in df.columns]\n",
    "    actors_narrow = df[keep_cols].copy()\n",
    "\n",
    "    # Pivot wide\n",
    "    actors_wide = (\n",
    "        actors_narrow\n",
    "        .set_index([movie_id_col, \"actor_idx\"])\n",
    "        .unstack(\"actor_idx\")\n",
    "    )\n",
    "\n",
    "    # Flatten MultiIndex columns: (feature, idx) -> f\"actor{idx}_{feature}\"\n",
    "    actors_wide.columns = [f\"actor{idx}_{name}\" for name, idx in actors_wide.columns]\n",
    "    actors_wide = actors_wide.reset_index()\n",
    "\n",
    "    # Merge back\n",
    "    wide_df = movie_base.merge(actors_wide, on=movie_id_col, how=\"left\")\n",
    "\n",
    "    # Order columns: movie fields first, then actors by idx\n",
    "    movie_cols_first = list(movie_base.columns)\n",
    "    actor_cols_after = sorted(\n",
    "        [c for c in wide_df.columns if c not in movie_cols_first],\n",
    "        key=lambda x: (int(x.split(\"_\")[0].replace(\"actor\", \"\")), x)\n",
    "    )\n",
    "    wide_df = wide_df[movie_cols_first + actor_cols_after]\n",
    "\n",
    "    return wide_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f9bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_columns(df, columns):\n",
    "    for col in columns:\n",
    "        encoder = sklearn.preprocessing.OneHotEncoder(sparse_output=False)\n",
    "        encoded = encoder.fit_transform(df[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]))\n",
    "        df = pd.concat([df.drop(col, axis=1), encoded_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506cd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_runtime(df, column_name='runtime'):\n",
    "    # Define bins (in minutes)\n",
    "    bins = [0, 90, 120, 150, float('inf')]\n",
    "    labels = ['short', 'medium', 'long', 'very_long']\n",
    "\n",
    "    # Create a new column for runtime category\n",
    "    df['runtime_category'] = pd.cut(df[column_name], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f86625",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c326116",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset = merging_tsv_files_data.drop(labels = [merging_tsv_files_data.columns[0], merging_tsv_files_data.columns[1], 'writers', 'characters', 'primaryProfession', 'birthYear', 'deathYear', 'known_for_movie_1', 'known_for_movie_2', 'known_for_movie_3', 'known_for_movie_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e930752",
   "metadata": {},
   "source": [
    "#### Calculating movie score and an actor sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611a52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset['movie_score'] = simpel_dataset['averageRating'] * simpel_dataset['numVotes']\n",
    "simpel_dataset['actor_sentiment'] = simpel_dataset['movie_score']/simpel_dataset['ordering']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77204d21",
   "metadata": {},
   "source": [
    "### Remove duplicate actors per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d977b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset = simpel_dataset.drop_duplicates(subset=['tconst', 'nconst'])\n",
    "simpel_dataset = simpel_dataset.sort_values(['tconst', 'ordering'])\n",
    "simpel_dataset['ordering'] = simpel_dataset.groupby('tconst').cumcount() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275628f",
   "metadata": {},
   "source": [
    "#### Calculating actors prior movie sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a602d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset['_orig_order'] = np.arange(len(simpel_dataset))  # to break same-date ties\n",
    "simpel_dataset = simpel_dataset.sort_values(['primaryName', 'startYear', '_orig_order'], kind='mergesort')\n",
    "simpel_dataset['prior_movie_actor_sentiment'] = simpel_dataset.groupby('primaryName')['actor_sentiment'].transform(lambda x: x.cumsum() - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25ff54",
   "metadata": {},
   "source": [
    "#### Converting strings to int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ee8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset['startYear'] = simpel_dataset['startYear'].replace('\\\\N', np.nan)\n",
    "simpel_dataset['startYear'] = pd.to_numeric(simpel_dataset['startYear'], errors='coerce')\n",
    "simpel_dataset['startYear'] = np.floor(simpel_dataset['startYear'])\n",
    "simpel_dataset['startYear'] = simpel_dataset['startYear'].astype('Int64')\n",
    "simpel_dataset['runtimeMinutes'] = simpel_dataset['runtimeMinutes'].replace('\\\\N', np.nan)\n",
    "simpel_dataset['runtimeMinutes'] = pd.to_numeric(simpel_dataset['runtimeMinutes'], errors='coerce')\n",
    "simpel_dataset['runtimeMinutes'] = np.floor(simpel_dataset['runtimeMinutes'])\n",
    "simpel_dataset['runtimeMinutes'] = simpel_dataset['runtimeMinutes'].astype('Int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdabe8",
   "metadata": {},
   "source": [
    "#### Calculating prior known for movies per actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229ebcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\AppData\\Local\\Temp\\ipykernel_5928\\13068810.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  aug = simpel_dataset.groupby('primaryName', group_keys=False).apply(cumulative_top3_prior)\n"
     ]
    }
   ],
   "source": [
    "simpel_dataset = simpel_dataset.sort_values(['primaryName', 'startYear', '_orig_order'], kind='mergesort')\n",
    "aug = simpel_dataset.groupby('primaryName', group_keys=False).apply(cumulative_top3_prior)\n",
    "simpel_dataset = simpel_dataset.join(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f05431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset.to_csv('..\\\\data\\\\interrim_data1.csv', sep =';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667dab",
   "metadata": {},
   "source": [
    "## One row per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4f073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_dataset = pd.read_csv('..\\\\data\\\\interrim_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b781c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8841e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = make_movie_actor_wide(simpel_dataset, max_actors=num_actors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c174a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1, num_actors + 1):\n",
    "    actor_pos = f'actor{i}_nconst'\n",
    "    actor_null_mask = wide_df[actor_pos].isna()\n",
    "    actor_non_null_mask = ~actor_null_mask\n",
    "    wide_df.loc[actor_null_mask, actor_pos] = -1\n",
    "    for suffix in ['_actor_sentiment', '_prior1_rating', '_prior2_rating', '_prior3_rating', '_prior_movie_actor_sentiment']:\n",
    "        col_name = f'actor{i}{suffix}'\n",
    "        if col_name in wide_df.columns:\n",
    "            wide_df.loc[actor_null_mask, col_name] = -1\n",
    "            wide_df.loc[actor_non_null_mask & wide_df[col_name].isna(), col_name] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887a17d",
   "metadata": {},
   "source": [
    "### One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9bf22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df['runtimeMinutes'] = wide_df['runtimeMinutes'].fillna(wide_df.groupby('startYear')['runtimeMinutes'].transform('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e283938",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = categorize_runtime(wide_df, 'runtimeMinutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb96339",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = one_hot_encode_columns(wide_df, ['genre_1', 'genre_2', 'genre_3', 'runtime_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc18c9",
   "metadata": {},
   "source": [
    "## Removing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94bce205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1, num_actors + 1):\n",
    "    actor_id = f'actor{i}_nconst'\n",
    "    actor_name = f'actor{i}_primaryName'\n",
    "    wide_df = wide_df.drop(columns=[actor_id, actor_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a81065ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'tconst', 'titleType', 'primaryTitle', 'originalTitle', 'directors', 'category', 'job']\n",
    "wide_df = wide_df.drop(columns=columns_to_drop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
