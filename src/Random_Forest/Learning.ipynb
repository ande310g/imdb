{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forest Model for Movie Score Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from scipy.stats import randint, uniform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Datasets\n",
        "Both training and test datasets are loaded.\n",
        "The test dataset is only used for evaluating the baseline random forest model, the final model is evaluated in the testing notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainingDatasetPath = os.path.join('.', 'data', 'training_dataset.csv')\n",
        "dataset_training = pd.read_csv(trainingDatasetPath, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "testDatasetPath = os.path.join('.', 'data', 'test_dataset.csv')\n",
        "df_test = pd.read_csv(testDatasetPath, sep=\";\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the Data\n",
        "The training and test datasets are prepared for the random forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = dataset_training['movie_score'].values\n",
        "dataset_training = dataset_training.drop(columns=['movie_score', 'Unnamed: 0', 'averageRating', 'numVotes', '_orig_order'])\n",
        "x_train = dataset_training.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test = df_test['movie_score'].values\n",
        "df_test = df_test.drop(columns=['movie_score', 'Unnamed: 0', 'averageRating', 'numVotes', '_orig_order'])\n",
        "x_test = df_test.values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create and Train Basic Random Forest Model\n",
        "Here a basic random forest model is trained, without any hyperparameter tuning. This will act as a baseline for the performance of the optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating and training a basic random forest model\n",
        "rf_basic = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_basic.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate basic model\n",
        "y_train_pred = rf_basic.predict(x_train)\n",
        "y_test_pred = rf_basic.predict(x_test)\n",
        "\n",
        "print(\"Basic Random Forest Model Performance:\")\n",
        "print(f\"Training R²: {r2_score(y_train, y_train_pred):.3f}\")\n",
        "print(f\"Test R²: {r2_score(y_test, y_test_pred):.3f}\")\n",
        "print(f\"Training RMSE: {np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(y_train_pred))):.3f}\")\n",
        "print(f\"Test RMSE: {np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_test_pred))):.3f}\")\n",
        "print(f\"Training MAE: {mean_absolute_error(np.expm1(y_train), np.expm1(y_train_pred)):.3f}\")\n",
        "print(f\"Test MAE: {mean_absolute_error(np.expm1(y_test), np.expm1(y_test_pred)):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning with RandomizedSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest with RandomizedSearchCV (much faster than GridSearchCV)\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1, max_samples=0.8, verbose=0)\n",
        "\n",
        "# Optimized parameter distributions for faster search\n",
        "param_dist_rf = {\n",
        "    # n_estimators: Number of decision trees in the random forest ensemble.\n",
        "    # More trees generally improve performance but increase training time.\n",
        "    # Values: 300 or 400 trees\n",
        "    'n_estimators': [300, 400],\n",
        "    \n",
        "    # max_depth: Maximum depth of each decision tree in the forest.\n",
        "    # Deeper trees can capture more complex patterns but may overfit.\n",
        "    # None means nodes are expanded until all leaves are pure or contain min_samples_split samples.\n",
        "    # Values: 10, 20, 30, or 40 levels deep\n",
        "    'max_depth': [10, 20, 30, 40],\n",
        "    \n",
        "    # min_samples_split: Minimum number of samples required to split an internal node.\n",
        "    # Higher values prevent overfitting by requiring more samples before splitting.\n",
        "    # Random integer between 2 and 20 (exclusive of 20)\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    \n",
        "    # min_samples_leaf: Minimum number of samples required to be at a leaf node.\n",
        "    # Higher values create more conservative trees and reduce overfitting.\n",
        "    # Random integer between 2 and 15 (exclusive of 15)\n",
        "    'min_samples_leaf': randint(2, 15),\n",
        "    \n",
        "    # max_features: Number of features to consider when looking for the best split.\n",
        "    # 'sqrt': Uses sqrt(n_features) features (common default for classification)\n",
        "    # 'log2': Uses log2(n_features) features (another common choice)\n",
        "    # This parameter controls the randomness and diversity of trees in the ensemble.\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "cv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    rf,\n",
        "    param_distributions=param_dist_rf,\n",
        "    n_iter=30, \n",
        "    cv=cv,\n",
        "    scoring='r2',\n",
        "    n_jobs=1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "rf_random_search.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = rf_random_search.best_estimator_\n",
        "\n",
        "# Refit best model on ALL data\n",
        "best_model.fit(x_train, y_train)\n",
        "# Save the model\n",
        "joblib.dump(best_model, \"../random_forest_reg_movie_log_transformed.joblib\")\n",
        "\n",
        "# Saves the best parameters and tried parameters to csv file\n",
        "cv_results = pd.DataFrame(rf_random_search.cv_results_)\n",
        "saveRandomForestRegressorCvResultsAt = os.path.join('..','..', 'data', 'random_forest_reg_movie_log_transformed.csv')\n",
        "cv_results.to_csv(saveRandomForestRegressorCvResultsAt, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect best Random Forest parameters and scores\n",
        "best_params_rf = rf_random_search.best_params_\n",
        "best_rf = rf_random_search.best_estimator_\n",
        "\n",
        "print(f\"Best Random Forest parameters: {best_params_rf}\")\n",
        "print(f\"Best Random Forest CV R-squared: {rf_random_search.best_score_:.3f}\")\n",
        "print(f\"Best Random Forest training R-squared: {best_rf.score(x_train, y_train):.3f}\")\n",
        "print(f\"Best Random Forest test R-squared: {best_rf.score(x_test, y_test):.3f}\")\n",
        "\n",
        "# Additional metrics\n",
        "y_train_pred_best = best_rf.predict(x_train)\n",
        "y_test_pred_best = best_rf.predict(x_test)\n",
        "\n",
        "print(f\"\\nBest Random Forest Model Performance:\")\n",
        "print(f\"Training RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred_best)):.3f}\")\n",
        "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred_best)):.3f}\")\n",
        "print(f\"Training MAE: {mean_absolute_error(y_train, y_train_pred_best):.3f}\")\n",
        "print(f\"Test MAE: {mean_absolute_error(y_test, y_test_pred_best):.3f}\")\n",
        "#Best Random Forest parameters: {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 400}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importances from the best model\n",
        "feature_importances = best_rf.feature_importances_\n",
        "feature_names = dataset_training.columns\n",
        "\n",
        "# Create a DataFrame for easier visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Display top 20 most important features\n",
        "print(\"Top 20 Most Important Features:\")\n",
        "print(importance_df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top 20 feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 20 Feature Importances - Random Forest')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Validation Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform cross-validation on the best model\n",
        "cv_scores = cross_val_score(best_rf, x_train, y_train, cv=10, scoring='r2', n_jobs=-1)\n",
        "print(f\"Cross-validation R² scores: {cv_scores}\")\n",
        "print(f\"Mean CV R²: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
