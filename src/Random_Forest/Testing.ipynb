{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352d33c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9fecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d789a1",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4b15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"movie_score\"\n",
    "drop_cols = [\"movie_score\", \"averageRating\", \"numVotes\", \"Unnamed: 0\", \"_orig_order\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac1160",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDatasetPath = os.path.join('.', 'data', 'test_dataset.csv')\n",
    "final_model = joblib.load(\"random_forest_reg_movie_log_transformed.joblib\")\n",
    "df_test = pd.read_csv(testDatasetPath, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b550a0",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data\n",
    "y_true = df_test[target_col].to_numpy()\n",
    "y_true_original = np.expm1(y_true)\n",
    "X_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46f348",
   "metadata": {},
   "source": [
    "### Create prediction on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_original = np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ff175",
   "metadata": {},
   "source": [
    "### Calculate MAE, RMSE and R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics (only if ground truth available)\n",
    "if y_true is not None:\n",
    "    mae = mean_absolute_error(y_true_original, y_pred_original)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2:   {r2:.4f}\")\n",
    "else:\n",
    "    print(\"Ground-truth 'movie_score' not found in test set; only predictions are available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e3d85",
   "metadata": {},
   "source": [
    "## Bin testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = [0, 10, 50, 100, 500, 1000, 5000, 10000, 20000, 40000, np.inf]\n",
    "bin_labels = [\"0-10\", \"10-50\", \"50-100\", \"100-500\", \"500-1000\", \"1000-5000\", \"5000-10000\", \"10000-20000\", \"20000-40000\", \"40000+\"]\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"y_true\": y_true_original,\n",
    "    \"y_pred\": y_pred_original\n",
    "})\n",
    "\n",
    "# Put each row into a bin based on TRUE score (recommended)\n",
    "eval_df[\"score_bin\"] = pd.cut(\n",
    "    eval_df[\"y_true\"],\n",
    "    bins=bin_edges,\n",
    "    labels=bin_labels,\n",
    "    right=False,          # [low, high)\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "def metrics_for_slice(y_t, y_p):\n",
    "    return {\n",
    "        \"n\": len(y_t),\n",
    "        \"mae\": mean_absolute_error(y_t, y_p),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_t, y_p)),\n",
    "        \"mean_true\": float(np.mean(y_t)),\n",
    "        \"mean_pred\": float(np.mean(y_p)),\n",
    "        \"median_abs_err\": float(np.median(np.abs(y_p - y_t))),\n",
    "    }\n",
    "\n",
    "bin_results = (\n",
    "    eval_df.dropna(subset=[\"score_bin\"])\n",
    "           .groupby(\"score_bin\", observed=True)\n",
    "           .apply(lambda g: pd.Series(metrics_for_slice(g[\"y_true\"].values, g[\"y_pred\"].values)))\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nPer-bin performance (binned by TRUE movie_score):\")\n",
    "print(bin_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results.to_csv(\"bin_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"pred_bin\"] = pd.cut(\n",
    "    eval_df[\"y_pred\"],\n",
    "    bins=bin_edges,\n",
    "    labels=bin_labels,\n",
    "    right=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "pred_bin_results = (\n",
    "    eval_df.dropna(subset=[\"pred_bin\"])\n",
    "           .groupby(\"pred_bin\", observed=True)\n",
    "           .apply(lambda g: pd.Series(metrics_for_slice(g[\"y_true\"].values, g[\"y_pred\"].values)))\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nPer-bin performance (binned by PREDICTED movie_score):\")\n",
    "print(pred_bin_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e988dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bin_results.to_csv('performance_binned_by_predicted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f926e1",
   "metadata": {},
   "source": [
    "### Create a preview dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview predictions\n",
    "preview = pd.DataFrame({\"predicted_movie_score\": y_pred, \"residual\": y_pred - y_true})\n",
    "if y_true is not None:\n",
    "    preview.insert(0, \"actual_movie_score\", y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c44a21",
   "metadata": {},
   "source": [
    "### Create a dataframe of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = final_model.feature_importances_\n",
    "feature_names = X_test.columns\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importances\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de312cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Feature Importances - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ee1b1",
   "metadata": {},
   "source": [
    "### Graph over residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_true\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_true, residuals, alpha=0.3)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"True popularity\")\n",
    "plt.ylabel(\"Residual (pred - true)\")\n",
    "plt.title(\"Residuals vs true popularity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2993a9",
   "metadata": {},
   "source": [
    "### Bias plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = y_pred / (y_true + 1e-9)\n",
    "plt.figure()\n",
    "plt.hist(np.log10(ratio), bins=50)\n",
    "plt.axvline(0)  # log10(1)\n",
    "plt.xlabel(\"log10(pred / true)\")\n",
    "plt.title(\"Prediction bias distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
