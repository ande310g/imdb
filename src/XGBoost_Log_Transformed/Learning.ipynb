{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8660fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bece70",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a192c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Unnamed: 0', 'averageRating', 'numVotes', '_orig_order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7b12c",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7596f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\".\\data\\training_dataset.csv\", sep=\";\")\n",
    "df = df.sort_values(by=['startYear', '_orig_order'])\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "y = df[\"movie_score\"].values\n",
    "X = df.drop(columns=[\"movie_score\"])  # + your drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c2a35",
   "metadata": {},
   "source": [
    "### Base model and parameter distribution for randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c076fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",   \n",
    "    device=\"cuda\" #NVIDIA gpu for training         \n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    # Number of boosting trees (iterations).\n",
    "    # More trees can improve performance but increase risk of overfitting and training time.\n",
    "    \"n_estimators\": randint(200, 2000),\n",
    "\n",
    "    # Step size shrinkage used in each boosting step.\n",
    "    # Lower values make learning more conservative and usually require more trees.\n",
    "    \"learning_rate\": uniform(0.01, 0.2),\n",
    "\n",
    "    # Maximum depth of each decision tree.\n",
    "    # Controls model complexity: deeper trees capture more interactions but may overfit.\n",
    "    \"max_depth\": randint(2, 10),\n",
    "\n",
    "    # Minimum sum of instance weight (Hessian) needed in a child node.\n",
    "    # Higher values make the algorithm more conservative and reduce overfitting.\n",
    "    \"min_child_weight\": randint(1, 10),\n",
    "\n",
    "    # Fraction of training samples used to grow each tree.\n",
    "    # Subsampling helps prevent overfitting and improves generalization.\n",
    "    \"subsample\": uniform(0.5, 0.5),         # samples between 50% and 100%\n",
    "\n",
    "    # Fraction of features (columns) used when constructing each tree.\n",
    "    # Reduces correlation between trees and helps control overfitting.\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),  # features between 50% and 100%\n",
    "\n",
    "    # L1 regularization term on weights (Lasso).\n",
    "    # Encourages sparsity by driving some leaf weights to zero.\n",
    "    \"reg_alpha\": uniform(0.0, 1.0),\n",
    "\n",
    "    # L2 regularization term on weights (Ridge).\n",
    "    # Penalizes large weights and stabilizes the model.\n",
    "    \"reg_lambda\": uniform(0.5, 2.0),\n",
    "\n",
    "    # Minimum loss reduction required to make a further split.\n",
    "    # Higher values make the model more conservative by limiting tree growth.\n",
    "    \"gamma\": uniform(0.0, 1.0),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390188d",
   "metadata": {},
   "source": [
    "### Tune the model for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ff563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation but keeping the temporal order of the data \n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#Randomized search to find the best hyperparamters based on param_dist\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9c8d0",
   "metadata": {},
   "source": [
    "### Train the best model with parameters and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc9d742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search.best_estimator_\n",
    "\n",
    "# Refit best model on ALL data\n",
    "best_model.fit(X, y, verbose=False)\n",
    "# Save the model\n",
    "joblib.dump(best_model, \"xgb_reg_movie_log_transformed.joblib\")\n",
    "\n",
    "# Saves the best parameters and tried parameters to csv file\n",
    "cv_results = pd.DataFrame(search.cv_results_)\n",
    "cv_results.to_csv(r\".\\data\\xgb_reg_movie_log_transformed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
