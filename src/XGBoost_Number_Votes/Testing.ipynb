{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352d33c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d789a1",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4b15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"movie_score\"\n",
    "drop_cols = [\"movie_score\", \"averageRating\", \"numVotes\", \"Unnamed: 0\", \"_orig_order\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac1160",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2f1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "useBackslash = True\n",
    "testDatasetPath = r'.\\data\\test_dataset.csv' if useBackslash else r'./data/test_dataset.csv'\n",
    "final_model = joblib.load(\"xgb_reg_movie_number_votes.joblib\")\n",
    "df_test = pd.read_csv(testDatasetPath, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b550a0",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data, transform the log scalled data back\n",
    "y_true = np.expm1(df_test[target_col].values)\n",
    "X_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46f348",
   "metadata": {},
   "source": [
    "### Create prediction on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and turn the prediction back to normal scale \n",
    "y_pred = np.expm1(final_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ff175",
   "metadata": {},
   "source": [
    "### Calculate MAE, RMSE and R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics (only if ground truth available)\n",
    "if y_true is not None:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(np.log1p(y_true), np.log1p(y_pred))\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2:   {r2:.4f}\")\n",
    "else:\n",
    "    print(\"Ground-truth 'movie_score' not found in test set; only predictions are available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652336d",
   "metadata": {},
   "source": [
    "## Bin testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bin-based evaluation ---\n",
    "bin_edges = [0, 10, 50, 100, 500, 1000, 5000, 10000, 20000, 40000, np.inf]\n",
    "bin_labels = [\"0-10\", \"10-50\", \"50-100\", \"100-500\", \"500-1000\", \"1000-5000\", \"5000-10000\", \"10000-20000\", \"20000-40000\", \"40000+\"]\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "\n",
    "# Put each row into a bin based on TRUE score (recommended)\n",
    "eval_df[\"score_bin\"] = pd.cut(\n",
    "    eval_df[\"y_true\"],\n",
    "    bins=bin_edges,\n",
    "    labels=bin_labels,\n",
    "    right=False,          # [low, high)\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "def metrics_for_slice(y_t, y_p):\n",
    "    return {\n",
    "        \"n\": len(y_t),\n",
    "        \"mae\": mean_absolute_error(y_t, y_p),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_t, y_p)),\n",
    "        \"mean_true\": float(np.mean(y_t)),\n",
    "        \"mean_pred\": float(np.mean(y_p)),\n",
    "        \"median_abs_err\": float(np.median(np.abs(y_p - y_t))),\n",
    "    }\n",
    "\n",
    "bin_results = (\n",
    "    eval_df.dropna(subset=[\"score_bin\"])\n",
    "           .groupby(\"score_bin\", observed=True)\n",
    "           .apply(lambda g: pd.Series(metrics_for_slice(g[\"y_true\"].values, g[\"y_pred\"].values)))\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nPer-bin performance (binned by TRUE movie_score):\")\n",
    "print(bin_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cdaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_results.to_csv(r'.\\data\\performance_binned_by_true.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"pred_bin\"] = pd.cut(\n",
    "    eval_df[\"y_pred\"],\n",
    "    bins=bin_edges,\n",
    "    labels=bin_labels,\n",
    "    right=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "pred_bin_results = (\n",
    "    eval_df.dropna(subset=[\"pred_bin\"])\n",
    "           .groupby(\"pred_bin\", observed=True)\n",
    "           .apply(lambda g: pd.Series(metrics_for_slice(g[\"y_true\"].values, g[\"y_pred\"].values)))\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nPer-bin performance (binned by PREDICTED movie_score):\")\n",
    "print(pred_bin_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e4e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bin_results.to_csv(r'.\\data\\performance_binned_by_predicted.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f926e1",
   "metadata": {},
   "source": [
    "### Create a preview dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview predictions\n",
    "preview = pd.DataFrame({\"predicted_movie_score\": y_pred, \"residual\": abs(y_pred - y_true)})\n",
    "if y_true is not None:\n",
    "    preview.insert(0, \"actual_movie_score\", y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c44a21",
   "metadata": {},
   "source": [
    "### Create a dataframe of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25e585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = final_model.feature_importances_\n",
    "\n",
    "# Feature names (if trained with a DataFrame)\n",
    "feature_names = final_model.feature_names_in_\n",
    "\n",
    "# Create sorted DataFrame\n",
    "features = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importances\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf23464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv(r'.\\data\\feature_importance.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ee1b1",
   "metadata": {},
   "source": [
    "### Graph over residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_true\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_true, residuals, alpha=0.3)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"True popularity\")\n",
    "plt.ylabel(\"Residual (pred - true)\")\n",
    "plt.title(\"Residuals vs true popularity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2993a9",
   "metadata": {},
   "source": [
    "### Bias plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = y_pred / (y_true + 1e-9)\n",
    "plt.figure()\n",
    "plt.hist(np.log10(ratio), bins=50)\n",
    "plt.axvline(0)  # log10(1)\n",
    "plt.xlabel(\"log10(pred / true)\")\n",
    "plt.title(\"Prediction bias distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
